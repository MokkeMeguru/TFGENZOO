[[https://github.com/MokkeMeguru/TFGENZOO/workflows/tensorflow%20test/badge.svg?branch=master]]
[[https://img.shields.io/badge/License-MIT-yellow.svg]]
[[file:https://img.shields.io/badge/python-3.7-blue.svg]]
[[file:https://img.shields.io/badge/tensorflow-%3E%3D2.2.0-brightgreen.svg]]
#+TITLE: TFGENZOO (Generative Model x Tensorflow 2.x)
* News [2020/6/16]
New training results [[https://www.tensorflow.org/datasets/catalog/oxford_flowers102][Oxford-flower102]] with only 8 hours! (Quadro P6000 x 1)

|------------------+-------------------+-------+------------|
| data             |         NLL(test) | epoch | pretrained |
|------------------+-------------------+-------+------------|
| Oxford-flower102 | 4.590211391448975 |  1024 | ---        |
|------------------+-------------------+-------+------------|

[[./docs/oxford.png]]

see more detail, you can see [[https://docs.google.com/presentation/d/12z6MZizIsytLxUb2ly7vYorFiKruIGZ2ckQ0-By4b6s/edit?usp=sharing][my internship's report]] (Japanese only, if you need translated version, please contact me.)
* News [2020/6/29]
Set Naming Convention

#+BEGIN_CENTER
XXX [YYY] [WithMask]
#+END_CENTER
- XXX is layer name such as Inv1x1Conv
- YYY is the data-dimention such as 2D. default 3D
- WithMask is the support of Mask

#+begin_example
Inv1x1Conv -> Inv1x1Conv for 3D Tensor
Inv1x1Conv2D -> Inv1x1Conv for 2D Tensor
Inv1x1Conv2DWithMask -> Inv1x1Conv for 2D Tensor with the support of Mask
#+end_example

** Why implement these layer?
1. Mask is a high burden of memory. Flow-based Model needs more memory in training, So we want to reduce mask as much as possible.
2. Mask often pollute the invertible characteristics.
3. Multi-dimensional Layer often have complex implementation. It makes proofing and verification difficult.
* What's this repository?
  This is a repository for some researcher to build some Generative models using Tensorflow 2.0.

  I NEED YOUR HELP(please let me know about formula, implementation and anything you worried)
* Zen of this repository
#+begin_example
We don't want to need flexible architectures.
We need strict definitions for shapes, parameters, and formulas.
We should Implement correct codes with well-documented(tested).
#+end_example

* How to use?
** by Install
- pipenv
   #+begin_src
   pipenv install git+https://github.com/MokkeMeguru/TFGENZOO.git@v1.2.4#egg=TFGENZOO
   # or pipenv install TFGENZOO
   #+end_src
  
- pip
   #+begin_src
   pip install git+https://github.com/MokkeMeguru/TFGENZOO.git@v1.2.4#egg=TFGENZOO
   # or pip install TFGENZOO
   #+end_src
** Source build for development

  1. clone this repository (If you want to do it, I will push this repository to PYPI)
  2. build this repository ~docker-compose build~
  3. run the environment ~sh run_script.sh~
  4. connect it via VSCode or Emacs or vi or anything.

* Examples
  - [[https://github.com/MokkeMeguru/TFGENZOO_EXAMPLE][TFGENZOO_EXAMPLE]]
* Roadmap
    - [X] Flow-based Model Architecture (RealNVP, Glow)
    - [ ] i-ResNet Model Architecture (i-ResNet, i-RevNet)
    - [ ] GANs Model Architecture (GANs)


* Backlog
** News [2020/3/15]
  I try to update correct loss function (training is correct, but nll is something wrong...)
** News [2020/2/28]
  I may implement normalizing flow.     
  You can try it with these commands in your shell.     
  And also, You can check training process via tensorboard in ~TFGENZOO/glow_log~
#+begin_src shell
sh run_script.sh
[docker]$ cd workspace/Github
[docker]$ python
python 3.6 > from TFGENZOO.examples.glow_mnist import trainer
python 3.6 > trainer.main()
#+end_src

** News [2020/3/17]

 update loss value
 |-------+-------+------------+--------------|
 | data  |   NLL(val) | epoch      | pretrained   |
 |-------+-------+------------+--------------|
 | MNIST | 1.56 | about 450 | --- |
 |-------+-------+------------+--------------|

 #+begin_src shell
 docker-compose build
 sh run_script.sh
 [docker]$ cd workspace/Github
 [docker]$ python
 python 3.6 > from TFGENZOO.examples.glow_mnist import trainer
 python 3.6 > trainer.main()
 #+end_src

 requirements
 - Nvidia-Docker
 - GPU > NVIDIA 1080
 - about 4 hours

** News [2020/4/24]
   publish installable alpha-version!!!

** News [2020/5/1]
  Move example code to the [[https://github.com/MokkeMeguru/TFGENZOO_EXAMPLE][TFGENZOO_EXAMPLE]]. 

** News [2020/5/25]

 update loss value in Glow-MNIST
 |-------+-------+------------+--------------|
 | data  |   NLL(val) | epoch      | pretrained   |
 |-------+-------+------------+--------------|
 | MNIST | 1.33 | 64 | --- |
 |-------+-------+------------+--------------|
** News [2020/5/29]
New training results [[https://www.tensorflow.org/datasets/catalog/oxford_flowers102][Oxford-flower102]] with only 4 hours! (Quadro P6000 x 1)

|------------------+-------------------+-------+------------|
| data             |          NLL(test) | epoch | pretrained |
|------------------+-------------------+-------+------------|
| Oxford-flower102 | 4.640194892883301 |   512 | ---        |
|------------------+-------------------+-------+------------|

[[https://github.com/MokkeMeguru/seminar/blob/master/TFGENZOO/512epoch.png]]

** News [2020/6/12]
- Implement SPADE Layer
- Implement Invertible Flatten Layer
- Update document with Some Example
* Contact
MokkeMeguru ([[https://twitter.com/MeguruMokke][@MokkeMeguru]]): DM or Mention Please (in Any language).
